\subsubsection{\stid{1.14} UPC++} 
\paragraph{Overview} 
The UPC++ project~\cite{upcxx-site} at LBNL is developing a C++ library
that supports Partitioned Global Address Space (PGAS) programming~\cite{Bachan:paw17,upcxx-spec}.
UPC++ is markedly different from an earlier prototype designated V0.1 \cite{zheng:ipdps14}.  
First, all communication is \emph{asynchronous}, to allow the overlap of computation and
communication, and to encourage programmers to avoid global synchronization. Second, all communication
is \emph{explicit}, to encourage programmers to consider the costs of communication. Third,
UPC++ encourages the use of \emph{scalable data-structures}
and avoids non-scalable library features.
%such as symmetric heaps. % this is too strong, inflammatory and not supported
All of these principles are intended to provide a programming model that can
scale efficiently to potentially millions of processors.
% We are revising the library under the auspices of the DOE's Exascale Computing
% Project, to meet the needs of applications requiring PGAS support.
UPC++ is well-suited for implementing elaborate distributed data structures where
communication is irregular or fine-grained. 
The UPC++ communication interfaces for Remote Memory Access (RMA) 
and Remote Procedure Calls (RPC)
are composable and fit naturally within the context of modern C++.

UPC++ is needed for ECP because it delivers low-overhead communication that runs
at close to hardware speeds, embracing 
interest by vendors in the PGAS model that
efficiently matches the RDMA mechanisms offered by
network hardware and on-chip communication between distinct address
spaces.  
Because ECP applications rely on irregular representations
to improve accuracy and conserve memory, the UPC++ library provides
an essential ingredient for the ECP software stack.  It enables
effective scaling of Exascale software by minimizing the work funneled
to lightweight cores, avoiding the overhead of long, branchy serial
code paths, and supporting efficient fine-grained communication.  The
importance of these properties is reinforced by application trends;
many ECP applications require the use of irregular data structures such as 
adaptive meshes, sparse
matrices, particles, or similar techniques, and also perform load balancing.  UPC++'s
low-overhead communication mechanisms can maximize injection rate and
network utilization, tolerate latency through overlap, streamline
unpredictable communication events, minimize synchronization, and
efficiently support small- to medium-sized messages arising in such
applications.  UPC++ enables the ECP software stack to exploit
the best-available communication mechanisms, including novel features
being developed by vendors.  This library offers a complementary,
yet interoperable, approach to MPI with OpenMP, enabling developers to
focus their effort on optimizing performance-critical communication.

\paragraph{Key  Challenges}

As a result of technological trends, the cost of data motion is steadily increasing relative to that of computation.  To reduce communication costs we need to 
either reduce the software overheads or hide  communication behind available computation. UPC++ addresses both strategies.
To reduce software overheads, UPC++ takes advantage of the GASNet-EX~\cite{gasnet-lcpc18,gasnet-site}
communication library's 
low-overhead communication as well as access to any special hardware
(see the accompanying report on GASNet-EX, which is being co-designed).
UPC++ supports asynchronous communication via classic one-sided communication
(i.e. puts and gets) and remote procedure calls.
%, to support communication hiding.


% A challenge in the ECP-ST effort is to maintain interoperability among run times,
% that invoke the back end to carry out communication. The difficulty is that each
% backend operates under the assumption that it "owns" the network.
% But many ECP applications employ (or will under ECP) multiple runtimes;
% thus interoperability is of paramount concern.
% Our approach is conservative; so long as entries and exits between different
% models is sufficiently coarse grained, then it is feasible to synchronize
% at a barrier at each transition.

\paragraph{Solution Strategy}

The UPC++ project has two primary thrusts:
\begin{enumerate}
\item \textbf{Increased performance through reduced communication costs:} The
UPC++ programmer can expect communication to run at close to hardware speeds.
Asynchronous execution enables an application to hide communication behind
available computation.

\item \textbf{Improved productivity:}  UPC++'s treatment of asynchronous
execution relies on futures and promises, and these simplify the management of
asynchrony.

\end{enumerate}

The PGAS one-sided communication employed by UPC++ (get/put)
benefits application  performance by mapping tightly onto the RDMA mechanisms
supported by the network hardware. GASNet-EX provides the
thin middleware
needed to enable this model to run at close to hardware speeds, across platforms ranging from laptops to supercomputers.
One-sided communication also has another benefit:
it decouples synchronization from data motion,
avoiding the fine-grained synchronization overheads of two-sided message-passing communication.

UPC++'s Remote Procedure Call (RPC)
%, which is built on GASNet Active Messages,
% provides additional control over asynchronous execution, by enabling
enables the programmer
to execute procedure calls on remote processors.
RPC is useful in managing access to complicated irregular data structures,
and in expressing asynchronous task execution, where communication patterns
are data-dependent and hence difficult to predict.

UPC++ addresses productivity via one-sided data motion, remote procedure calls,
and future-based management of asynchrony.
Futures enable the programmer
to capture data readiness state,
which is useful in making scheduling decisions,
and to attach handlers that 
execute asynchronously as dependencies become
satisfied. Futures may also be conjoined, to aggregate
completion processing. The outcome is to avoid explicit synchronization of completion at a fine granularity.



\paragraph{Recent Progress}

We have implemented a distributed hash table,
which is used by the Exabiome Project (2.2.4.04), that scales to efficiency
to large number of processors. RPC was observed to simplify the implementation
considerably, by avoiding data hazards without the need for locking.
Figure~\ref{fig:dht} illustrates the benefits of the UPC++ model 
in a weak scaling study on the KNL partition of NERSC's Cori.


%\newlength{\dhtsz}


\begin{figure}[htb]
\centering
%  \subfloat[Cori Haswell\label{fig:dht-haswell-wait}]{ \includegraphics{projects/2.3.1-PMR/2.3.1.14-UPCxx-GASNet/all-cori-haswell-out-inserts-wait.png} }
%  \subfloat[Cori KNL\label{fig:dht-knl-wait}]{}
      \includegraphics{projects/2.3.1-PMR/2.3.1.14-UPCxx-GASNet/all-cori-knl-out-inserts-wait.png}
  \caption{Weak scaling of distributed hash table insertion on the KNL partition of NERSC's Cori platform. The dotted line represents the processes in one node.}
  \label{fig:dht}
\end{figure}



\paragraph{Next Steps}

\begin{enumerate}
\item \textbf{Memory Kinds.}
Unified abstractions for 
transferring data back and forth between device (e.g. GPU) and host memory, possibly remote.
By unifying the means of expressing data transfer among the collective memories of a heterogeneous system with different memory kinds,
the abstractions enable ECP applications to utilize accelerators, which are necessary to attain peak performance on ECP platforms.
The design and abstraction enables eventual hardware offload (such as to GPUDirect) of device data transfers, support for which will be provided by future work in GASNet-EX.

\item \textbf{Sustainability.}
Ensure that UPC++ remains viable as new compiler versions are released into the ECP software stack. UPC++ benefits to productivity rely heavily on template meta-programming, using features added in C+11.  However not all currently available compilers comply sufficiently with the C+11 standard.

\end{enumerate}
